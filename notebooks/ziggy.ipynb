{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb1d23b-1fa9-4112-8501-c68d42b48f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0.dev20241112)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Collecting onnx\n",
      "  Using cached onnx-1.17.0-cp312-cp312-macosx_12_0_universal2.whl.metadata (16 kB)\n",
      "Collecting onnxruntime\n",
      "  Using cached onnxruntime-1.20.1-cp312-cp312-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/anaconda3/lib/python3.12/site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/anaconda3/lib/python3.12/site-packages (from onnx) (3.20.3)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached onnx-1.17.0-cp312-cp312-macosx_12_0_universal2.whl (16.7 MB)\n",
      "Using cached onnxruntime-1.20.1-cp312-cp312-macosx_13_0_universal2.whl (31.0 MB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Installing collected packages: flatbuffers, sympy, onnx, humanfriendly, coloredlogs, onnxruntime\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-24.3.25 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.20.1 sympy-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch tiktoken onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f78c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tiktoken\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "158e63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise tiktoken tokeniser\n",
    "tokeniser = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def encode_text(text, max_length):\n",
    "    tokens = tokeniser.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    if len(tokens) > max_length:\n",
    "        tokens = tokens[:max_length]  # Truncate\n",
    "    else:\n",
    "        tokens += [0] * (max_length - len(tokens))  # Pad\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93882a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        input_ids = torch.tensor(encode_text(text, self.max_length), dtype=torch.long)\n",
    "        attention_mask = (input_ids != 0).long()  # Mask non-padding tokens\n",
    "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300f638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, num_classes, max_seq_length):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, embed_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.embedding(input_ids) + self.positional_encoding[:, :input_ids.size(1), :]\n",
    "        transformer_output = self.transformer_encoder(\n",
    "            embedded.transpose(0, 1),  # (seq_len, batch, embed_dim)\n",
    "            src_key_padding_mask=~attention_mask.bool()  # Inverse mask\n",
    "        )\n",
    "        pooled_output = transformer_output.mean(dim=0)  # Mean pooling\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d3c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = tokeniser.n_vocab  # Tokeniser vocabulary size\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "num_classes = 3  # Example: 3 classes\n",
    "max_seq_length = 128\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d620f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs, learning_rate, device):\n",
    "    model = model.to(device)\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            input_ids, attention_mask, labels = (\n",
    "                input_ids.to(device),\n",
    "                attention_mask.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6df86b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.1631\n",
      "Epoch 2/5, Loss: 0.9526\n",
      "Epoch 3/5, Loss: 0.8484\n",
      "Epoch 4/5, Loss: 0.7075\n",
      "Epoch 5/5, Loss: 0.5841\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "texts = [\"I love programming\", \"Python is great\", \"I hate bugs\"]\n",
    "labels = [0, 1, 2]  # Example labels\n",
    "dataset = TextClassificationDataset(texts, labels, max_seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialise model\n",
    "model = TransformerClassifier(vocab_size, embed_dim, num_heads, num_layers, num_classes, max_seq_length)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train model\n",
    "train_model(model, dataloader, epochs, learning_rate, device)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"/Users/paulzanna/Github/Ziggy/model/ziggy_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83701ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Export ONNX model\n",
    "#\n",
    "dummy_input_ids = torch.randint(0, vocab_size, (1, max_seq_length)).to(device)\n",
    "dummy_attention_mask = torch.ones(1, max_seq_length).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input_ids, dummy_attention_mask),\n",
    "    \"text_classifier.onnx\",\n",
    "    opset_version=14,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"seq_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"seq_length\"},\n",
    "        \"logits\": {0: \"batch_size\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ae660d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Verify ONNX model\n",
    "#\n",
    "\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Load ONNX model\n",
    "ort_session = ort.InferenceSession(\"text_classifier.onnx\")\n",
    "\n",
    "# Run inference\n",
    "def predict_with_onnx(ort_session, input_ids, attention_mask):\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids.cpu().numpy(),\n",
    "        \"attention_mask\": attention_mask.cpu().numpy(),\n",
    "    }\n",
    "    logits = ort_session.run(None, inputs)[0]\n",
    "    return np.argmax(logits, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
