{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1d23b-1fa9-4112-8501-c68d42b48f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch tiktoken onnx onnxruntime panadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f78c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import onnxruntime as ort\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "158e63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise tiktoken tokeniser\n",
    "tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def encode_text(text, max_length):\n",
    "    tokens = tokeniser.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    if len(tokens) > max_length:\n",
    "        tokens = tokens[:max_length]  # Truncate\n",
    "    else:\n",
    "        tokens += [0] * (max_length - len(tokens))  # Pad\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f6865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = tokeniser.n_vocab  # Tokeniser vocabulary size\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "num_layers = 6\n",
    "max_seq_length = 512\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# Config\n",
    "model_path = \"/Users/paulzanna/Github/Ziggy/model/\"\n",
    "model_filename = \"ziggy_model.bin\"\n",
    "onnx_model_filename = \"ziggy_model.onnx\"\n",
    "data_path = \"/Users/paulzanna/Github/Ziggy/data/\"\n",
    "data_filename = \"data.csv\"\n",
    "req_filename = \"requirements.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93882a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        input_ids = torch.tensor(encode_text(text, self.max_length), dtype=torch.long)\n",
    "        attention_mask = (input_ids != 0).long()  # Mask non-padding tokens\n",
    "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300f638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, num_classes, max_seq_length):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, embed_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.embedding(input_ids) + self.positional_encoding[:input_ids.size(1), :]\n",
    "        transformer_output = self.transformer_encoder(\n",
    "            embedded.transpose(0, 1),  # (seq_len, batch, embed_dim)\n",
    "            src_key_padding_mask=~attention_mask.bool()  # Inverse mask\n",
    "        )\n",
    "        pooled_output = transformer_output.mean(dim=0)  # Mean pooling\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d620f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs, learning_rate, device):\n",
    "    model = model.to(device)\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            input_ids, attention_mask, labels = (\n",
    "                input_ids.to(device),\n",
    "                attention_mask.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff32189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels and create mappings\n",
    "labels = pd.read_csv(data_path + req_filename)\n",
    "print(labels)\n",
    "id2label = pd.Series(labels.requirement.values, index=labels.id).to_dict()\n",
    "label2id = pd.Series(labels.id.values, index=labels.requirement).to_dict()\n",
    "num_classes = len(id2label)\n",
    "\n",
    "# print labels\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example data\n",
    "clause_data = pd.read_csv(data_path + data_filename)\n",
    "\n",
    "# Combine label columns into a single multi-label 'label' column\n",
    "label_columns = labels['requirement'].tolist()\n",
    "clause_data['label'] = clause_data[label_columns].values.tolist()\n",
    "clause_data = clause_data.drop(columns=label_columns)\n",
    "\n",
    "# Find which item in clause is the label\n",
    "clause_data['label'] = clause_data['label'].apply(lambda x: [i for i, v in enumerate(x) if v == 1])\n",
    "clauses = clause_data['clause'].tolist()\n",
    "clause_label = clause_data['label'].apply(lambda x: x[0] if x else -1)  # Convert to single integer label or -1 if empty\n",
    "clause_label = clause_label.to_list()\n",
    "# Print clauses and labels\n",
    "print(clauses)\n",
    "print(clause_label)\n",
    "\n",
    "# Convert clauses to a dataset\n",
    "dataset = TextClassificationDataset(clauses, clause_label, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df86b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialise model\n",
    "model = TransformerClassifier(vocab_size, embed_dim, num_heads, num_layers, num_classes, max_seq_length)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Train model\n",
    "train_model(model, dataloader, epochs, learning_rate, device)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), model_path + model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = TransformerClassifier(vocab_size, embed_dim, num_heads, num_layers, num_classes, max_seq_length)\n",
    "model_hf= torch.load(model_path + model_filename)\n",
    "sd_hf = model.state_dict()\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_hf[\"positional_encoding\"].view(-1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sd_hf[\"embedding.weight\"], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c29b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"positional_encoding\"][:, 150])\n",
    "plt.plot(sd_hf[\"positional_encoding\"][:, 200])\n",
    "plt.plot(sd_hf[\"positional_encoding\"][:, 250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83701ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Export ONNX model\n",
    "#\n",
    "dummy_input_ids = torch.randint(0, vocab_size, (1, max_seq_length)).to(device)\n",
    "dummy_attention_mask = torch.ones(1, max_seq_length).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input_ids, dummy_attention_mask),\n",
    "    model_path + onnx_model_filename,\n",
    "    opset_version=14,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"seq_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"seq_length\"},\n",
    "        \"logits\": {0: \"batch_size\"},\n",
    "    },\n",
    "    input_types=[torch.int64, torch.int64]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae660d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Verify ONNX model\n",
    "#\n",
    "\n",
    "# Function to predict using ONNX\n",
    "def predict_with_onnx(ort_session, input_ids, attention_mask):\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids.cpu().numpy().astype(np.int64),  # Ensure int64 type\n",
    "        \"attention_mask\": attention_mask.astype(np.float32),\n",
    "    }\n",
    "    logits = ort_session.run(None, inputs)[0]\n",
    "    return np.argmax(logits, axis=1)\n",
    "\n",
    "# Load ONNX model\n",
    "ort_session = ort.InferenceSession(model_path + onnx_model_filename)\n",
    "\n",
    "# Input text\n",
    "input_text = \"The service provider must ensure that all data is encrypted at rest\"\n",
    "\n",
    "# Tokenise and preprocess\n",
    "input_ids = torch.tensor([encode_text(input_text, max_seq_length)], dtype=torch.int64)\n",
    "attention_mask = (input_ids != 0).numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "# Apply softmax to logits to compute probabilities\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))  # Subtract max for numerical stability\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "# Predict and compute probabilities\n",
    "logits = ort_session.run(None, {\"input_ids\": input_ids.cpu().numpy().astype(np.int64), \"attention_mask\": attention_mask.astype(np.float32)})[0]\n",
    "\n",
    "# Apply softmax\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "# Get predicted label and probability\n",
    "predicted_label = np.argmax(probabilities, axis=1)[0]\n",
    "predicted_probability = probabilities[0][predicted_label]\n",
    "\n",
    "# Print the id2label mapping of the predicted label\n",
    "print(f\"Predicted Label: {id2label[predicted_label]}\")\n",
    "print(f\"Probability: {predicted_probability * 100:.2f}%\")\n",
    "print(f\"Probabilities: {probabilities[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
